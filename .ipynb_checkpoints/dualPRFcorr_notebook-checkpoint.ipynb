{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual-PRF outlier identification and correction\n",
    "Uses circular mean to identify outliers and corrects them based on the deviation from the local median velocity.\n",
    "The methodology is less affected by neighbouring outliers and can be applied in the presence of global aliasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as plb\n",
    "import matplotlib as mpl\n",
    "import pyart\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import numpy.ma as ma\n",
    "import re\n",
    "\n",
    "from pylab import *\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary function: \n",
    "Adds dummy columns (with NA values) in range for wrapping radar image edges when applying the convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dummy_cols(data, kernel, val='nan'):\n",
    "    \n",
    "    # Image boundaries are wrapped for calculation of convolution\n",
    "    # NA values (zeroes/nan) are added in range\n",
    "    \n",
    "    c = (np.asarray(kernel.shape)-1)/2\n",
    "    add_cols = ceil(c[1])\n",
    "    dummy_cols = np.zeros((data.shape[0], add_cols.astype(int)))\n",
    "    \n",
    "    if val=='nan':\n",
    "        dummy_cols[:] = np.NAN\n",
    "    else:\n",
    "        dummy_cols[:] = val\n",
    "        \n",
    "    # Add dummy columns\n",
    "    data_out = np.hstack((data, dummy_cols))\n",
    "    \n",
    "    return add_cols, data_out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary function: \n",
    "Returns an array with the number of local neighbours with a valid velocity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def local_valid(mask, kernel=np.ones((3,3))):\n",
    "    \n",
    "    ## Calculate number of local neighbours with valid value ##\n",
    "    \n",
    "    # Modified mask (NA values addition)\n",
    "    mask_tmp = (~mask).astype(int)\n",
    "    ncols, mask_tmp = dummy_cols(mask_tmp, kernel, val=0)\n",
    "    \n",
    "    # Convolve with kernel to calculate number of valid neighbours\n",
    "    valid_tmp = ndimage.convolve(mask_tmp, kernel, mode='wrap')\n",
    "    \n",
    "    # Remove added values\n",
    "    valid = valid_tmp[:, : (valid_tmp.shape[1] - ncols)]\n",
    "    \n",
    "    return valid.astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary function: \n",
    "Returns an array with the dual-PRF factor corresponding to each gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def primary_vel(dim, Nprf, prf_flag=None, prf_odd=None):\n",
    "    \n",
    "    ## Construct array with the dual-PRF factor corresponding to each gate ##\n",
    "    # Flag 0 indicates low PRF and flag 1 indicates high PRF\n",
    "    \n",
    "    flag_vec = np.ones(dim[0])\n",
    "    \n",
    "    if (prf_flag is None) & (prf_odd is not None) :\n",
    "        if prf_odd==0:\n",
    "            flag_vec[::2] = 0\n",
    "        elif prf_odd==1:\n",
    "            flag_vec[1::2] = 0\n",
    "    else:\n",
    "        flag_vec = prf_flag\n",
    "        \n",
    "    flag_vec = flag_vec - 1\n",
    "    flag_vec[flag_vec<0] = 1\n",
    "        \n",
    "    flag_arr = np.transpose(np.tile(flag_vec, (dim[1], 1)))\n",
    "    Nprf_arr = flag_arr + Nprf\n",
    "    \n",
    "    return Nprf_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary function: \n",
    "Returns an array with either the mean or the median velocity calculated for the neighbouring gates in a user-defined window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ref_val(data, mask, kernel, method='median'):\n",
    "    \n",
    "    dummy_val=0\n",
    "    Nval_arr = np.ones(mask.shape)\n",
    "    \n",
    "    if method=='mean':\n",
    "        Nval_arr = local_valid(mask, kernel=kernel)\n",
    "        dummy_data = data*(~mask).astype(int)\n",
    "        \n",
    "    if method=='median':\n",
    "        dummy_val='nan'\n",
    "        dummy_data = np.where(np.logical_not(mask), data, np.nan)\n",
    "        \n",
    "\n",
    "    ncols, data_conv = dummy_cols(dummy_data, kernel, val=dummy_val)\n",
    "    \n",
    "    if method=='mean':\n",
    "        conv_arr = ndimage.convolve(data_conv, weights=kernel, mode='wrap')\n",
    "        \n",
    "    if method=='median':\n",
    "        conv_arr = ndimage.generic_filter(data_conv, np.nanmedian, footprint=kernel, mode='wrap')\n",
    "    \n",
    "    # Remove added columns and divide by weight\n",
    "    conv_arr = conv_arr[:, : (conv_arr.shape[1] - ncols)]\n",
    "    ref_arr = conv_arr/Nval_arr\n",
    "    \n",
    "    return ref_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification function using circular statistics:\n",
    "Returns an array with the reference velocities to be used in the correction function and a mask-like array indicating the dual-PRF outliers. Reference velocities are the median velocity calculated for the neighbouring gates after identified outliers have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outlier_detector_cmean(np_ma, Vny, Nprf_arr, Nmin=2):\n",
    "    \n",
    "    data = np_ma.data\n",
    "    mask = np_ma.mask\n",
    "    \n",
    "    f_arr = np.ones(Nprf_arr.shape)\n",
    "    f_arr[np.where(Nprf_arr==np.min(Nprf_arr))] = -1\n",
    "    Vny_arr = Vny/Nprf_arr\n",
    "    \n",
    "    kH, kL = np.zeros((5,5)), np.zeros((5,5))\n",
    "    kH[1::2] = 1\n",
    "    kL[::2] = 1\n",
    "     \n",
    "    # Array with the number of valid neighbours at each point\n",
    "    Nval_arr_H = local_valid(mask, kernel=kH)\n",
    "    Nval_arr_L = local_valid(mask, kernel=kL)\n",
    "    \n",
    "    # Convert to angles and calculate trigonometric variables\n",
    "    ang_ma = (np_ma*pi/Vny)\n",
    "    cos_ma = ma.cos(ang_ma*Nprf_arr)\n",
    "    sin_ma = ma.sin(ang_ma*Nprf_arr)\n",
    "    \n",
    "    # Average trigonometric variables in local neighbourhood\n",
    "    dummy_cos = cos_ma.data*(~mask).astype(int)\n",
    "    dummy_sin = sin_ma.data*(~mask).astype(int)\n",
    "    \n",
    "    ncols, cos_conv = dummy_cols(dummy_cos, kH, val=0)\n",
    "    ncols, sin_conv = dummy_cols(dummy_sin, kH, val=0)\n",
    "    \n",
    "    cos_sumH = ndimage.convolve(cos_conv, weights=kH, mode='wrap')\n",
    "    cos_sumL = ndimage.convolve(cos_conv, weights=kL, mode='wrap')\n",
    "    \n",
    "    sin_sumH = ndimage.convolve(sin_conv, weights=kH, mode='wrap')\n",
    "    sin_sumL = ndimage.convolve(sin_conv, weights=kL, mode='wrap')\n",
    "    \n",
    "    # Remove added columns\n",
    "    cos_sumH = cos_sumH[:, : (cos_sumL.shape[1] - ncols)]\n",
    "    cos_sumL = cos_sumL[:, : (cos_sumL.shape[1] - ncols)]\n",
    "    sin_sumH = sin_sumH[:, : (sin_sumL.shape[1] - ncols)]\n",
    "    sin_sumL = sin_sumL[:, : (sin_sumL.shape[1] - ncols)]\n",
    "    \n",
    "    # Average angle in local neighbourhood\n",
    "    cos_avgH_ma = ma.array(data=cos_sumH, mask=mask)/Nval_arr_H\n",
    "    cos_avgL_ma = ma.array(data=cos_sumL, mask=mask)/Nval_arr_L\n",
    "    sin_avgH_ma = ma.array(data=sin_sumH, mask=mask)/Nval_arr_H\n",
    "    sin_avgL_ma = ma.array(data=sin_sumL, mask=mask)/Nval_arr_L\n",
    "      \n",
    "    BH = ma.arctan2(sin_avgH_ma, cos_avgH_ma)\n",
    "    BL = ma.arctan2(sin_avgL_ma, cos_avgL_ma)\n",
    "    \n",
    "    # Average velocity ANGLE of neighbours (reference ANGLE for outlier detection):\n",
    "    angref_ma = f_arr*(BL-BH)\n",
    "    angref_ma[angref_ma<0] = angref_ma[angref_ma<0] + 2*pi\n",
    "    angref_ma[angref_ma>pi] = - (2*pi - angref_ma[angref_ma>pi])\n",
    "    angobs_ma = ma.arctan2(ma.sin(ang_ma), ma.cos(ang_ma))\n",
    "    \n",
    "    # Detector array (minimum ANGLE difference between observed and reference):\n",
    "    diff = angobs_ma - angref_ma\n",
    "    det_ma = (Vny/pi)*ma.arctan2(ma.sin(diff), ma.cos(diff))\n",
    "    \n",
    "    out_mask = np.zeros(det_ma.shape)\n",
    "    out_mask[abs(det_ma)>0.8*Vny_arr] = 1\n",
    "    out_mask[(Nval_arr_H<Nmin)|(Nval_arr_L<Nmin)] = 0\n",
    "    \n",
    "    # CORRECTION\n",
    "    \n",
    "    # Convolution kernel\n",
    "    kernel = np.ones(kH.shape)\n",
    "    \n",
    "    new_mask = (mask) | (out_mask.astype(bool))\n",
    "    \n",
    "    # Array with the number of valid neighbours at each point (outliers removed)\n",
    "    Nval_arr = local_valid(new_mask, kernel=kernel)\n",
    "    \n",
    "    out_mask[Nval_arr<Nmin] = 0\n",
    "    \n",
    "    ref_arr = ref_val(data, new_mask, kernel, method='median')\n",
    "    ref_ma = ma.array(data=ref_arr, mask=mask)\n",
    "    \n",
    "    return ref_ma, out_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correction function:\n",
    "This is the main function. \n",
    "Estimates the unwrap number that results in the minimum deviation between the gate velocity and the corresponding reference velocity. Returns a masked array with the corrected velocities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_dualPRF_cmean(radar, field='velocity', \n",
    "                   corr_method='median', Nmin=2):\n",
    "    \n",
    "    v_ma = radar.fields[field]['data']\n",
    "    vcorr_ma = v_ma.copy()\n",
    "    out_mask = np.zeros(v_ma.shape)\n",
    "    \n",
    "    # Dual-PRF parameters\n",
    "    Vny = radar.instrument_parameters['nyquist_velocity']['data'][0] #Nyquist velocity\n",
    "    prf_flag = radar.instrument_parameters['prf_flag']['data'] #flag for identifying the PRF of the radial\n",
    "    f = radar.instrument_parameters['prt_ratio']['data'][0] \n",
    "    Nprf = int(round(1/(f-1))) #dual-PRF factor\n",
    "    \n",
    "    # Array with the primary Nyquist velocity corresponding to each bin\n",
    "    Nprf_arr = primary_vel(v_ma.shape, Nprf, prf_flag=prf_flag)\n",
    "    Vny_arr = Vny/Nprf_arr\n",
    "    \n",
    "    for nsweep, sweep_slice in enumerate(radar.iter_slice()):\n",
    "        \n",
    "        v0 = v_ma[sweep_slice] # velocity field\n",
    "        vp = Vny_arr[sweep_slice] # primary velocities\n",
    "        nprfp = Nprf_arr[sweep_slice] # dual-PRF factors\n",
    "        \n",
    "        ref, out_mask[sweep_slice] = outlier_detector_cmean(v0, Vny, nprfp, Nmin=Nmin)\n",
    "                 \n",
    "        # Convert non-outliers to zero for correction procedure  \n",
    "        v0_out = v0*out_mask[sweep_slice]\n",
    "        vp_out = vp*out_mask[sweep_slice]\n",
    "        ref_out = ref*out_mask[sweep_slice]\n",
    "        vp_out_L = vp_out.copy() # Only low PRF outliers\n",
    "        vp_out_L[nprfp==Nprf] = 0\n",
    "        \n",
    "        dev = ma.abs(v0_out-ref_out)\n",
    "        nuw = np.zeros(v0.shape) # Number of unwraps (initialisation)\n",
    "        \n",
    "        for ni in range(-Nprf, (Nprf+1)):\n",
    "            \n",
    "            # New velocity values for identified outliers\n",
    "            if abs(ni)==Nprf:\n",
    "                vcorr_out = v0_out + 2*ni*vp_out_L\n",
    "            else:\n",
    "                vcorr_out = v0_out + 2*ni*vp_out\n",
    "            \n",
    "            # New deviation for new velocity values\n",
    "            dev_tmp = ma.abs(vcorr_out-ref_out)\n",
    "            # Compare with previous\n",
    "            delta = dev-dev_tmp\n",
    "            # Update unwrap number\n",
    "            nuw[delta>0] = ni\n",
    "            # Update corrected velocity and deviation\n",
    "            vcorr_out_tmp = v0_out + 2*nuw*vp_out\n",
    "            dev = ma.abs(vcorr_out_tmp-ref_out)\n",
    "            \n",
    "        # Corrected velocity field\n",
    "        vcorr_ma[sweep_slice] = v0 + 2*nuw*vp\n",
    "        \n",
    "    return vcorr_ma, out_mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
